{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXbDg4JWRgrB"
      },
      "source": [
        "# HybridTwoWay Model (Colab Ready)\n"
      ],
      "id": "lXbDg4JWRgrB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzI1FsIbRgrC"
      },
      "source": [
        "## Imports\n",
        "필요한 PyTorch 모듈과 타입 힌트를 불러옵니다."
      ],
      "id": "hzI1FsIbRgrC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUS-djHwRv7K",
        "outputId": "0a6eaf5d-8dbf-419d-8c25-3a1c7cae1d31"
      },
      "id": "gUS-djHwRv7K",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cpu)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (12.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow-avif-plugin, filetype, python-dotenv, pi-heif, opencv-python-headless, opencv-python, idna, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-4.12.0.88 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 python-dotenv-1.2.1 requests-toolbelt-1.0.0 roboflow-1.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nn_y7inWRgrC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "id": "nn_y7inWRgrC"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "roboflow-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roboflow-cell",
        "outputId": "4dd9a77b-dc11-48df-c5a7-eba9aba348fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in detection-base-6 to yolov8:: 100%|██████████| 111006/111006 [00:01<00:00, 79402.89it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to detection-base-6 in yolov8:: 100%|██████████| 3306/3306 [00:00<00:00, 8306.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roboflow dataset downloaded to: /content/detection-base-6\n"
          ]
        }
      ],
      "source": [
        "# --- 옵션 3: Roboflow 데이터셋 사용 --- #\n",
        "# !pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm3A0nt0RgrD"
      },
      "source": [
        "## 0. Utility Functions"
      ],
      "id": "sm3A0nt0RgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VmonqZbIRgrD"
      },
      "outputs": [],
      "source": [
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n"
      ],
      "id": "VmonqZbIRgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0hLza4pRgrD"
      },
      "source": [
        "## 1. Anomaly-Aware CNN Stem"
      ],
      "id": "u0hLza4pRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P1mfLvC-RgrD"
      },
      "outputs": [],
      "source": [
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'), self.weight, groups=self.groups)\n",
        "\n",
        "\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * 48\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ],
      "id": "P1mfLvC-RgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqpyko_eRgrD"
      },
      "source": [
        "## 2. Vision Transformer Encoder"
      ],
      "id": "Bqpyko_eRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rbrER0bmRgrD"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed1x1(nn.Module):\n",
        "    \"\"\"Map CNN features to ViT embeddings while keeping spatial resolution.\"\"\"\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n"
      ],
      "id": "rbrER0bmRgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbtIlHMRgrD"
      },
      "source": [
        "## 3. Feedback Adapter"
      ],
      "id": "hAbtIlHMRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yunrRMjJRgrE"
      },
      "outputs": [],
      "source": [
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, Ht: int, Wt: int, f_stem: torch.Tensor):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ],
      "id": "yunrRMjJRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEUDTPghRgrE"
      },
      "source": [
        "## 4. PAN-Lite Neck"
      ],
      "id": "mEUDTPghRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iiMIL_IoRgrE"
      },
      "outputs": [],
      "source": [
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        # p3 → mid\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "\n",
        "        # mid(80×80) → mid(40×40)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "\n",
        "        # optional: 강화 conv\n",
        "        self.refine4 = conv_bn_act(mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        \"\"\"\n",
        "        input:  p3  = [B, in_ch, H, W] (ex: 80x80)\n",
        "        output: p4  = [B, mid, H/2, W/2] (ex: 40x40)\n",
        "        \"\"\"\n",
        "        # lateral transform\n",
        "        p3 = self.lateral(p3)\n",
        "\n",
        "        # downsample once → P4\n",
        "        p4 = self.down4(p3)\n",
        "\n",
        "        # optional refinement\n",
        "        p4 = self.refine4(p4)\n",
        "\n",
        "        return p4\n"
      ],
      "id": "iiMIL_IoRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhcT-kPCRgrE"
      },
      "source": [
        "## 5. YOLO-style Detection Head"
      ],
      "id": "qhcT-kPCRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ckEXIyNcRgrE"
      },
      "outputs": [],
      "source": [
        "class YOLOHeadLite_P4Only(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "\n",
        "        # P4 전용 stem\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "\n",
        "        # P4 detection layers\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        f = self.stem4(x)\n",
        "        return self.cls4(f), self.obj4(f), self.box4(f)\n",
        "\n",
        "    def forward(self, p4):\n",
        "        c4, o4, b4 = self.forward_single(p4)\n",
        "        return [(c4, o4, b4)]     # 리스트 형태 유지 (학습 코드 호환)\n"
      ],
      "id": "ckEXIyNcRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Jz1d2sRgrE"
      },
      "source": [
        "## 6. HybridTwoWay Model"
      ],
      "id": "Z9Jz1d2sRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IK54_9JNRgrE"
      },
      "outputs": [],
      "source": [
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_ch=3,\n",
        "        stem_base=32,\n",
        "        embed_dim=256,\n",
        "        vit_depth=4,\n",
        "        vit_heads=4,\n",
        "        num_classes=3,\n",
        "        iters=1,\n",
        "        detach_feedback=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert iters >= 1\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4\n",
        "        self.patch = PatchEmbed1x1(c_stem, embed_dim)\n",
        "        self.vit = ViTEncoder(embed_dim=embed_dim, depth=vit_depth, num_heads=vit_heads)\n",
        "        self.feedback = FeedbackAdapter(embed_dim, c_stem, use_bn=True)\n",
        "        self.neck = PANLite(in_ch=embed_dim, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        f_stem, vis = self.stem(x)\n",
        "        p = self.patch(f_stem)\n",
        "        Ht, Wt = p.shape[-2:]\n",
        "        tokens = p.flatten(2).transpose(1, 2)\n",
        "        tokens = self.vit(tokens)\n",
        "        toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "        f_fb = self.feedback(toks_for_fb, Ht, Wt, f_stem)\n",
        "        p3_in = self.patch(f_fb)\n",
        "        p3 = p3_in\n",
        "        p4 = self.neck(p3)\n",
        "        all_preds = self.head([p4])\n",
        "        preds = [all_preds[0]]\n",
        "        aux = {\"P4\": p4, \"V\": vis}\n",
        "        return preds, aux, f_fb\n",
        "\n",
        "    def forward(self, x):\n",
        "        preds, aux, f_fb = self.forward_once(x)\n",
        "        for _ in range(self.iters - 1):\n",
        "            preds, aux, f_fb = self.forward_once(x)\n",
        "        return preds, aux\n"
      ],
      "id": "IK54_9JNRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b16ccb66",
      "metadata": {
        "id": "b16ccb66"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "IMG_SIZE = 512\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)   # tensor of shape [num_boxes, 5]\n",
        "\n",
        "    # stack images → OK\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "\n",
        "    # targets는 stack 안 함 (num_boxes 다르기 때문)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.images[idx]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = torch.tensor(img).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, x, y, w, h = map(float, line.split())\n",
        "                    boxes.append([cls, x, y, w, h])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        return img, boxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4e66cb6a",
      "metadata": {
        "id": "4e66cb6a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DATA_PATH = dataset.location\n",
        "\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"))\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=yolo_collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def yolo_loss(preds, targets, img_size=512):\n",
        "    object_loss = 0\n",
        "    class_loss = 0\n",
        "    box_loss = 0\n",
        "\n",
        "    for scale_id, (cls_pred, obj_pred, box_pred) in enumerate(preds):\n",
        "        B, num_cls, H, W = cls_pred.shape\n",
        "\n",
        "        # ----- flatten -----\n",
        "        cls_pred = cls_pred.permute(0,2,3,1).reshape(B, -1, num_cls)     # [B, H*W, C]\n",
        "        obj_pred = obj_pred.permute(0,2,3,1).reshape(B, -1, 1)           # [B, H*W, 1]\n",
        "        box_pred = box_pred.permute(0,2,3,1).reshape(B, -1, 4)           # [B, H*W, 4]\n",
        "\n",
        "        obj_pred = obj_pred.sigmoid()\n",
        "        cls_pred = cls_pred.sigmoid()\n",
        "\n",
        "        # ----- iterate batch -----\n",
        "        for b in range(B):\n",
        "            gt = targets[b]\n",
        "\n",
        "            if len(gt) == 0:\n",
        "                # object 없음 → obj는 0이어야 함\n",
        "                object_loss += (obj_pred[b] ** 2).mean()\n",
        "                continue\n",
        "\n",
        "            # object 있음 → obj는 1이어야 함\n",
        "            object_loss += ((1 - obj_pred[b]) ** 2).mean()\n",
        "\n",
        "            # ----- class loss -----\n",
        "            tcls = gt[:, 0].long()        # class index\n",
        "            class_loss += F.cross_entropy(\n",
        "                cls_pred[b],\n",
        "                tcls.expand(cls_pred[b].shape[0]),\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            # ----- box loss (L1) -----\n",
        "            # GT scaled to pixels\n",
        "            tbox = gt[:, 1:].clone()\n",
        "            tbox = tbox * img_size        # [xywh normalized → pixel]\n",
        "\n",
        "            pred_box = box_pred[b][:len(gt)] * img_size\n",
        "\n",
        "            box_loss += F.l1_loss(pred_box, tbox, reduction='mean')\n",
        "\n",
        "    total = object_loss + class_loss + box_loss\n",
        "    return total\n"
      ],
      "metadata": {
        "id": "kiW_wK3rWJOp"
      },
      "id": "kiW_wK3rWJOp",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fb248c1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "fb248c1f",
        "outputId": "450328ef-4923-48a5-87c8-94c81b30e33c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2183234983.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHybridTwoWay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimple_yolo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# 일단 파이프라인 연결용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \"\"\"\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \"\"\"\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "model = HybridTwoWay(num_classes=3).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def simple_yolo_loss(preds, targets):\n",
        "    return preds[0][0].mean() * 0  # 일단 파이프라인 연결용\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, targets in train_loader:\n",
        "        imgs = imgs.cuda()\n",
        "\n",
        "        preds, aux = model(imgs)\n",
        "\n",
        "        loss = simple_yolo_loss(preds, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | loss {total_loss:.4f}\")\n",
        "torch.save(model.state_dict(), \"hybrid_two_way_best.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=48,\n",
        "    embed_dim=512,\n",
        "    vit_depth=8,\n",
        "    vit_heads=8,\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=True\n",
        ").cuda()\n",
        "\n",
        "model.load_state_dict(torch.load(\"hybrid_two_way_best.pt\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "xMGJ7hNzVp1k"
      },
      "id": "xMGJ7hNzVp1k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1DpYDnPRgrE"
      },
      "source": [
        "## 7. Quick Sanity Check\n",
        "Colab에서 바로 실행해 모델 입출력 형태를 확인할 수 있습니다."
      ],
      "id": "U1DpYDnPRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvJstXD8RgrE",
        "outputId": "76923e41-24f3-4342-8db0-6250e4829840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TwoWay] P3 cls:[2, 1, 80, 80] obj:[2, 1, 80, 80] box:[2, 4, 80, 80]\n",
            "[TwoWay] P4 cls:[2, 1, 40, 40] obj:[2, 1, 40, 40] box:[2, 4, 40, 40]\n",
            "[TwoWay] P5 cls:[2, 1, 20, 20] obj:[2, 1, 20, 20] box:[2, 4, 20, 20]\n"
          ]
        }
      ],
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=48,\n",
        "    embed_dim=512,\n",
        "    vit_depth=8,\n",
        "    vit_heads=8,\n",
        "    num_classes=1,\n",
        "    iters=1,\n",
        "    detach_feedback=True,\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640)\n",
        "preds, aux = model(x)\n",
        "for i, (c, o, b) in enumerate(preds, start=3):\n",
        "    print(f\"[TwoWay] P{i} cls:{list(c.shape)} obj:{list(o.shape)} box:{list(b.shape)}\")\n"
      ],
      "id": "rvJstXD8RgrE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}