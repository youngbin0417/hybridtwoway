{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXbDg4JWRgrB"
      },
      "source": [
        "# HybridTwoWay Model (Colab Ready)\n"
      ],
      "id": "lXbDg4JWRgrB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzI1FsIbRgrC"
      },
      "source": [
        "## Imports\n",
        "필요한 PyTorch 모듈과 타입 힌트를 불러옵니다."
      ],
      "id": "hzI1FsIbRgrC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gUS-djHwRv7K",
        "outputId": "b5f2509a-f375-4449-8348-bb6304235cae"
      },
      "id": "gUS-djHwRv7K",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nn_y7inWRgrC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "id": "nn_y7inWRgrC"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "roboflow-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roboflow-cell",
        "outputId": "8f5c0973-33e8-40ad-970c-d3d64c93ef14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in detection-base-6 to yolov8:: 100%|██████████| 111006/111006 [00:07<00:00, 15311.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to detection-base-6 in yolov8:: 100%|██████████| 3306/3306 [00:00<00:00, 5772.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roboflow dataset downloaded to: /content/detection-base-6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 옵션 3: Roboflow 데이터셋 사용 --- #\n",
        "# !pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm3A0nt0RgrD"
      },
      "source": [
        "## 0. Utility Functions"
      ],
      "id": "sm3A0nt0RgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VmonqZbIRgrD"
      },
      "outputs": [],
      "source": [
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n"
      ],
      "id": "VmonqZbIRgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0hLza4pRgrD"
      },
      "source": [
        "## 1. Anomaly-Aware CNN Stem"
      ],
      "id": "u0hLza4pRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P1mfLvC-RgrD"
      },
      "outputs": [],
      "source": [
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'), self.weight, groups=self.groups)\n",
        "\n",
        "\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * 48\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ],
      "id": "P1mfLvC-RgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqpyko_eRgrD"
      },
      "source": [
        "## 2. Vision Transformer Encoder"
      ],
      "id": "Bqpyko_eRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rbrER0bmRgrD"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed1x1(nn.Module):\n",
        "    \"\"\"Map CNN features to ViT embeddings while keeping spatial resolution.\"\"\"\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n"
      ],
      "id": "rbrER0bmRgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbtIlHMRgrD"
      },
      "source": [
        "## 3. Feedback Adapter"
      ],
      "id": "hAbtIlHMRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yunrRMjJRgrE"
      },
      "outputs": [],
      "source": [
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, Ht: int, Wt: int, f_stem: torch.Tensor):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ],
      "id": "yunrRMjJRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEUDTPghRgrE"
      },
      "source": [
        "## 4. PAN-Lite Neck"
      ],
      "id": "mEUDTPghRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iiMIL_IoRgrE"
      },
      "outputs": [],
      "source": [
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        # p3 → mid\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "\n",
        "        # mid(80×80) → mid(40×40)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "\n",
        "        # optional: 강화 conv\n",
        "        self.refine4 = conv_bn_act(mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        \"\"\"\n",
        "        input:  p3  = [B, in_ch, H, W] (ex: 80x80)\n",
        "        output: p4  = [B, mid, H/2, W/2] (ex: 40x40)\n",
        "        \"\"\"\n",
        "        # lateral transform\n",
        "        p3 = self.lateral(p3)\n",
        "\n",
        "        # downsample once → P4\n",
        "        p4 = self.down4(p3)\n",
        "\n",
        "        # optional refinement\n",
        "        p4 = self.refine4(p4)\n",
        "\n",
        "        return p4\n"
      ],
      "id": "iiMIL_IoRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhcT-kPCRgrE"
      },
      "source": [
        "## 5. YOLO-style Detection Head"
      ],
      "id": "qhcT-kPCRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ckEXIyNcRgrE"
      },
      "outputs": [],
      "source": [
        "class YOLOHeadLite(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "\n",
        "        # P4 전용 stem\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "\n",
        "        # P4 detection layers\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        f = self.stem4(x)\n",
        "        return self.cls4(f), self.obj4(f), self.box4(f)\n",
        "\n",
        "    def forward(self, p4):\n",
        "        c4, o4, b4 = self.forward_single(p4)\n",
        "        return [(c4, o4, b4)]     # 리스트 형태 유지 (학습 코드 호환)\n"
      ],
      "id": "ckEXIyNcRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Jz1d2sRgrE"
      },
      "source": [
        "## 6. HybridTwoWay Model"
      ],
      "id": "Z9Jz1d2sRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IK54_9JNRgrE"
      },
      "outputs": [],
      "source": [
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_ch=3,\n",
        "        stem_base=32,\n",
        "        embed_dim=256,\n",
        "        vit_depth=4,\n",
        "        vit_heads=4,\n",
        "        num_classes=3,\n",
        "        iters=1,\n",
        "        detach_feedback=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert iters >= 1\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4\n",
        "        self.patch = PatchEmbed1x1(c_stem, embed_dim)\n",
        "        self.vit = ViTEncoder(embed_dim=embed_dim, depth=vit_depth, num_heads=vit_heads)\n",
        "        self.feedback = FeedbackAdapter(embed_dim, c_stem, use_bn=True)\n",
        "        self.neck = PANLite(in_ch=embed_dim, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        f_stem, vis = self.stem(x)\n",
        "        p = self.patch(f_stem)\n",
        "        Ht, Wt = p.shape[-2:]\n",
        "        tokens = p.flatten(2).transpose(1, 2)\n",
        "        tokens = self.vit(tokens)\n",
        "        toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "        f_fb = self.feedback(toks_for_fb, Ht, Wt, f_stem)\n",
        "        p3_in = self.patch(f_fb)\n",
        "        p3 = p3_in\n",
        "        p4 = self.neck(p3)\n",
        "\n",
        "        # [수정된 부분] self.head에 p4 텐서를 직접 전달합니다.\n",
        "        all_preds = self.head(p4)\n",
        "\n",
        "        preds = [all_preds[0]]\n",
        "        aux = {\"P4\": p4, \"V\": vis}\n",
        "        return preds, aux, f_fb\n",
        "\n",
        "    def forward(self, x):\n",
        "        preds, aux, f_fb = self.forward_once(x)\n",
        "        for _ in range(self.iters - 1):\n",
        "            preds, aux, f_fb = self.forward_once(x)\n",
        "        return preds, aux"
      ],
      "id": "IK54_9JNRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b16ccb66",
      "metadata": {
        "id": "b16ccb66"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "IMG_SIZE = 512\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)   # tensor of shape [num_boxes, 5]\n",
        "\n",
        "    # stack images → OK\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "\n",
        "    # targets는 stack 안 함 (num_boxes 다르기 때문)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.images[idx]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = torch.tensor(img).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, x, y, w, h = map(float, line.split())\n",
        "                    boxes.append([cls, x, y, w, h])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        return img, boxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4e66cb6a",
      "metadata": {
        "id": "4e66cb6a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DATA_PATH = dataset.location\n",
        "\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"))\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=yolo_collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_loss(preds, targets, img_size=512):\n",
        "    cls_pred, obj_pred, box_pred = preds[0]   # only P4\n",
        "\n",
        "    B, C, H, W = cls_pred.shape\n",
        "    device = cls_pred.device\n",
        "\n",
        "    # reshape\n",
        "    cls_pred = cls_pred.permute(0,2,3,1).reshape(B, H*W, C)\n",
        "    obj_pred = obj_pred.permute(0,2,3,1).reshape(B, H*W, 1)\n",
        "    box_pred = box_pred.permute(0,2,3,1).reshape(B, H*W, 4)\n",
        "\n",
        "    # losses\n",
        "    obj_loss = 0.0\n",
        "    cls_loss = 0.0\n",
        "    box_loss = 0.0\n",
        "\n",
        "    # stride\n",
        "    stride = img_size // H     # 512/32 = 16\n",
        "\n",
        "    for b in range(B):\n",
        "        gt = targets[b]\n",
        "        if len(gt) == 0:\n",
        "            # no objects → obj should be 0\n",
        "            obj_loss += F.binary_cross_entropy_with_logits(obj_pred[b], torch.zeros_like(obj_pred[b]))\n",
        "            continue\n",
        "\n",
        "        # assign GT to grid cell\n",
        "        gcls = gt[:, 0].long()\n",
        "        gxy  = gt[:, 1:3] * img_size       # center (pixels)\n",
        "        gwh  = gt[:, 3:5] * img_size       # w,h (pixels)\n",
        "\n",
        "        # grid index\n",
        "        gx = (gxy[:, 0] / stride).long().clamp(0, W-1)\n",
        "        gy = (gxy[:, 1] / stride).long().clamp(0, H-1)\n",
        "        gi = gy * W + gx                   # flatten index\n",
        "\n",
        "        # -------------------------------\n",
        "        # objectness loss\n",
        "        # -------------------------------\n",
        "        obj_tgt = torch.zeros((H*W,1), device=device)\n",
        "        obj_tgt[gi] = 1.0\n",
        "        obj_loss += F.binary_cross_entropy_with_logits(obj_pred[b], obj_tgt)\n",
        "\n",
        "        # -------------------------------\n",
        "        # class loss\n",
        "        # -------------------------------\n",
        "        cls_tgt = torch.zeros((H*W, C), device=device)\n",
        "        cls_tgt[gi, gcls] = 1.0\n",
        "        cls_loss += F.binary_cross_entropy_with_logits(cls_pred[b], cls_tgt)\n",
        "\n",
        "        # -------------------------------\n",
        "        # box loss (L1)\n",
        "        # -------------------------------\n",
        "        # box_pred is raw, model must output normalized xywh (0~1)\n",
        "        # if not, scale accordingly\n",
        "        pred = box_pred[b][gi]     # num_gt x 4\n",
        "        tgt  = torch.cat([gxy, gwh], dim=1) / img_size   # normalize to 0~1\n",
        "\n",
        "        box_loss += F.l1_loss(pred, tgt, reduction='mean')\n",
        "\n",
        "    total = obj_loss + cls_loss + box_loss\n",
        "    return total\n"
      ],
      "metadata": {
        "id": "kiW_wK3rWJOp"
      },
      "id": "kiW_wK3rWJOp",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fb248c1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb248c1f",
        "outputId": "b86419f1-4109-4d47-a69e-f4a2f5983a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 1384/1384 [01:34<00:00, 14.60it/s, loss=0.323]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Average Loss: 0.5823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 1384/1384 [01:34<00:00, 14.71it/s, loss=0.147]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Average Loss: 0.1743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 1384/1384 [01:34<00:00, 14.70it/s, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Average Loss: 0.1460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 1384/1384 [01:34<00:00, 14.70it/s, loss=0.0901]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Average Loss: 0.1355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 1384/1384 [01:34<00:00, 14.69it/s, loss=0.16]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Average Loss: 0.1283\n",
            "학습 완료!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# IMG_SIZE가 정의되어 있는지 확인\n",
        "if 'IMG_SIZE' not in globals():\n",
        "    IMG_SIZE = 512\n",
        "\n",
        "model = HybridTwoWay(num_classes=3).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # tqdm을 사용하여 진행률 표시\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # train_loader를 tqdm으로 감쌉니다.\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for imgs, targets in loop:\n",
        "        imgs = imgs.cuda()\n",
        "        # [수정된 부분] targets 리스트의 각 텐서를 cuda로 이동시킵니다.\n",
        "        targets = [t.cuda() for t in targets]\n",
        "\n",
        "        preds, aux = model(imgs)\n",
        "\n",
        "        # [수정된 부분] 실제 손실 함수 yolo_loss를 사용합니다.\n",
        "        loss = yolo_loss(preds, targets, img_size=IMG_SIZE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # tqdm에 현재 손실 표시\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"학습 완료!\")\n",
        "torch.save(model.state_dict(), \"hybrid_two_way_best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=32,\n",
        "    embed_dim=256,\n",
        "    vit_depth=4,\n",
        "    vit_heads=4,\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=True\n",
        ").cuda()\n",
        "\n",
        "model.load_state_dict(torch.load(\"hybrid_two_way_best.pt\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMGJ7hNzVp1k",
        "outputId": "c3e672b5-f4ff-4cef-9158-d14a69b166c7"
      },
      "id": "xMGJ7hNzVp1k",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HybridTwoWay(\n",
              "  (stem): AnomalyAwareStem(\n",
              "    (stem): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (blur): FixedGaussianBlur()\n",
              "    (anom): Sequential(\n",
              "      (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (4): SiLU(inplace=True)\n",
              "    )\n",
              "    (fuse): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (fuse_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (vis_head): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (patch): PatchEmbed1x1(\n",
              "    (proj): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (vit): ViTEncoder(\n",
              "    (blocks): ModuleList(\n",
              "      (0-3): 4 x TransformerBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiheadSelfAttention(\n",
              "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (feedback): FeedbackAdapter(\n",
              "    (adapter): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (neck): PANLite(\n",
              "    (lateral): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "    (down4): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "    (refine4): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (head): YOLOHeadLite(\n",
              "    (stem4): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "    (cls4): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (obj4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (box4): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1DpYDnPRgrE"
      },
      "source": [
        "## 7. Quick Sanity Check\n",
        "Colab에서 바로 실행해 모델 입출력 형태를 확인할 수 있습니다."
      ],
      "id": "U1DpYDnPRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvJstXD8RgrE",
        "outputId": "1cd62982-9bd6-4f05-a0b6-753c10b8ae72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TwoWay] P3 cls:[2, 3, 40, 40] obj:[2, 1, 40, 40] box:[2, 4, 40, 40]\n"
          ]
        }
      ],
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=32,\n",
        "    embed_dim=256,\n",
        "    vit_depth=4,\n",
        "    vit_heads=4,\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=True\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640)\n",
        "preds, aux = model(x)\n",
        "for i, (c, o, b) in enumerate(preds, start=3):\n",
        "    print(f\"[TwoWay] P{i} cls:{list(c.shape)} obj:{list(o.shape)} box:{list(b.shape)}\")\n"
      ],
      "id": "rvJstXD8RgrE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}