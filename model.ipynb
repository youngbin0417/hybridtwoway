{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXbDg4JWRgrB"
      },
      "source": [
        "# HybridTwoWay Model (Colab Ready)\n"
      ],
      "id": "lXbDg4JWRgrB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzI1FsIbRgrC"
      },
      "source": [
        "## Imports\n",
        "필요한 PyTorch 모듈과 타입 힌트를 불러옵니다."
      ],
      "id": "hzI1FsIbRgrC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUS-djHwRv7K",
        "outputId": "6fa1aba0-8087-471c-e471-53415d0b13e3"
      },
      "id": "gUS-djHwRv7K",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.12/dist-packages (1.2.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nn_y7inWRgrC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "id": "nn_y7inWRgrC"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "roboflow-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roboflow-cell",
        "outputId": "1c1be307-0a54-4879-d754-cd638d268147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Roboflow dataset downloaded to: /content/detection-base-6\n"
          ]
        }
      ],
      "source": [
        "# --- 옵션 3: Roboflow 데이터셋 사용 --- #\n",
        "# !pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm3A0nt0RgrD"
      },
      "source": [
        "## 0. Utility Functions"
      ],
      "id": "sm3A0nt0RgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VmonqZbIRgrD"
      },
      "outputs": [],
      "source": [
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n"
      ],
      "id": "VmonqZbIRgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0hLza4pRgrD"
      },
      "source": [
        "## 1. Anomaly-Aware CNN Stem"
      ],
      "id": "u0hLza4pRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P1mfLvC-RgrD"
      },
      "outputs": [],
      "source": [
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'), self.weight, groups=self.groups)\n",
        "\n",
        "\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * 48\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ],
      "id": "P1mfLvC-RgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqpyko_eRgrD"
      },
      "source": [
        "## 2. Vision Transformer Encoder"
      ],
      "id": "Bqpyko_eRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rbrER0bmRgrD"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed1x1(nn.Module):\n",
        "    \"\"\"Map CNN features to ViT embeddings while keeping spatial resolution.\"\"\"\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n"
      ],
      "id": "rbrER0bmRgrD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbtIlHMRgrD"
      },
      "source": [
        "## 3. Feedback Adapter"
      ],
      "id": "hAbtIlHMRgrD"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yunrRMjJRgrE"
      },
      "outputs": [],
      "source": [
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, Ht: int, Wt: int, f_stem: torch.Tensor):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ],
      "id": "yunrRMjJRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEUDTPghRgrE"
      },
      "source": [
        "## 4. PAN-Lite Neck"
      ],
      "id": "mEUDTPghRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iiMIL_IoRgrE"
      },
      "outputs": [],
      "source": [
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.down5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.up4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.up3 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse5 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        p3 = self.lateral(p3)\n",
        "        p4 = self.down4(p3)\n",
        "        p5 = self.down5(p4)\n",
        "        p4u = F.interpolate(p5, size=p4.shape[-2:], mode='nearest')\n",
        "        p4 = self.up4(torch.cat([p4, p4u], dim=1))\n",
        "        p3u = F.interpolate(p4, size=p3.shape[-2:], mode='nearest')\n",
        "        p3 = self.up3(torch.cat([p3, p3u], dim=1))\n",
        "        p4b = self.down_f4(p3)\n",
        "        p4 = self.fuse4(torch.cat([p4, p4b], dim=1))\n",
        "        p5b = self.down_f5(p4)\n",
        "        p5 = self.fuse5(torch.cat([p5, p5b], dim=1))\n",
        "        return p3, p4, p5\n"
      ],
      "id": "iiMIL_IoRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhcT-kPCRgrE"
      },
      "source": [
        "## 5. YOLO-style Detection Head"
      ],
      "id": "qhcT-kPCRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ckEXIyNcRgrE"
      },
      "outputs": [],
      "source": [
        "class YOLOHeadLite(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1, reg_max=0):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "        self.stem3 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem5 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.cls3 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj3 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box3 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        self.cls5 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj5 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box5 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "\n",
        "    def forward_single(self, x, stem, cls, obj, box):\n",
        "        f = stem(x)\n",
        "        return cls(f), obj(f), box(f)\n",
        "\n",
        "    def forward(self, p3, p4, p5):\n",
        "        c3, o3, b3 = self.forward_single(p3, self.stem3, self.cls3, self.obj3, self.box3)\n",
        "        c4, o4, b4 = self.forward_single(p4, self.stem4, self.cls4, self.obj4, self.box4)\n",
        "        c5, o5, b5 = self.forward_single(p5, self.stem5, self.cls5, self.obj5, self.box5)\n",
        "        return [(c3, o3, b3), (c4, o4, b4), (c5, o5, b5)]\n"
      ],
      "id": "ckEXIyNcRgrE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Jz1d2sRgrE"
      },
      "source": [
        "## 6. HybridTwoWay Model"
      ],
      "id": "Z9Jz1d2sRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IK54_9JNRgrE"
      },
      "outputs": [],
      "source": [
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_ch=3,\n",
        "        stem_base=32,\n",
        "        embed_dim=256,\n",
        "        vit_depth=4,\n",
        "        vit_heads=4,\n",
        "        num_classes=3,\n",
        "        iters=1,\n",
        "        detach_feedback=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert iters >= 1\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4\n",
        "        self.patch = PatchEmbed1x1(c_stem, embed_dim)\n",
        "        self.vit = ViTEncoder(embed_dim=embed_dim, depth=vit_depth, num_heads=vit_heads)\n",
        "        self.feedback = FeedbackAdapter(embed_dim, c_stem, use_bn=True)\n",
        "        self.neck = PANLite(in_ch=embed_dim, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        f_stem, vis = self.stem(x)\n",
        "        p = self.patch(f_stem)\n",
        "        Ht, Wt = p.shape[-2:]\n",
        "        tokens = p.flatten(2).transpose(1, 2)\n",
        "        tokens = self.vit(tokens)\n",
        "        toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "        f_fb = self.feedback(toks_for_fb, Ht, Wt, f_stem)\n",
        "        p3_in = self.patch(f_fb)\n",
        "        p3 = p3_in\n",
        "        p3, p4, p5 = self.neck(p3)\n",
        "        preds = self.head(p3, p4, p5)\n",
        "        aux = {\"P3\": p3, \"P4\": p4, \"P5\": p5, \"V\": vis}\n",
        "        return preds, aux, f_fb\n",
        "\n",
        "    def forward(self, x):\n",
        "        preds, aux, f_fb = self.forward_once(x)\n",
        "        for _ in range(self.iters - 1):\n",
        "            preds, aux, f_fb = self.forward_once(x)\n",
        "        return preds, aux\n"
      ],
      "id": "IK54_9JNRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b16ccb66",
      "metadata": {
        "id": "b16ccb66"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "IMG_SIZE = 512\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)   # tensor of shape [num_boxes, 5]\n",
        "\n",
        "    # stack images → OK\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "\n",
        "    # targets는 stack 안 함 (num_boxes 다르기 때문)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.images[idx]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = torch.tensor(img).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, x, y, w, h = map(float, line.split())\n",
        "                    boxes.append([cls, x, y, w, h])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        return img, boxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4e66cb6a",
      "metadata": {
        "id": "4e66cb6a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DATA_PATH = dataset.location\n",
        "\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"))\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=yolo_collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def yolo_loss(preds, targets, img_size=512):\n",
        "    object_loss = 0\n",
        "    class_loss = 0\n",
        "    box_loss = 0\n",
        "\n",
        "    for scale_id, (cls_pred, obj_pred, box_pred) in enumerate(preds):\n",
        "        B, N, _ = obj_pred.shape\n",
        "\n",
        "        # ----- sigmoid for predictions -----\n",
        "        obj_pred = obj_pred.sigmoid()\n",
        "        cls_pred = cls_pred.sigmoid()\n",
        "\n",
        "        # ----- 매우 단순화된 YOLO matching -----\n",
        "        # 여기서는 \"해당 이미지에 object가 있으면 obj=1\" 그냥 기본 체크\n",
        "        for b in range(B):\n",
        "            num_boxes = len(targets[b])\n",
        "            if num_boxes == 0:\n",
        "                # object 없음 → obj=0에 가까워야 함\n",
        "                object_loss += (obj_pred[b] ** 2).mean()\n",
        "                continue\n",
        "\n",
        "            # object 있음 → obj=1 근처여야 함\n",
        "            object_loss += ((1 - obj_pred[b]) ** 2).mean()\n",
        "\n",
        "            # class loss (1 클래스일 때)\n",
        "            tcls = targets[b][:, 0].long()\n",
        "            class_loss += F.cross_entropy(cls_pred[b], tcls, reduction=\"mean\")\n",
        "\n",
        "            # box loss (L1)\n",
        "            # normalize YOLO xywh → pixel 단위 좌표 가능\n",
        "            tbox = targets[b][:, 1:].clone()  # xywh (0~1)\n",
        "            tbox[:, 0] *= img_size\n",
        "            tbox[:, 1] *= img_size\n",
        "            tbox[:, 2] *= img_size\n",
        "            tbox[:, 3] *= img_size\n",
        "\n",
        "            pred_box = box_pred[b][:num_boxes]\n",
        "            pred_box = pred_box * img_size  # normalize → pixel\n",
        "\n",
        "            box_loss += F.l1_loss(pred_box, tbox, reduction=\"mean\")\n",
        "\n",
        "    total = object_loss + class_loss + box_loss\n",
        "    return total\n"
      ],
      "metadata": {
        "id": "kiW_wK3rWJOp"
      },
      "id": "kiW_wK3rWJOp",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "fb248c1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb248c1f",
        "outputId": "cccc3ce8-a2c4-4a8e-deef-40dc605b014a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | loss 0.0000\n",
            "Epoch 2 | loss 0.0000\n",
            "Epoch 3 | loss 0.0000\n"
          ]
        }
      ],
      "source": [
        "model = HybridTwoWay(num_classes=3).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, targets in train_loader:\n",
        "        imgs = imgs.cuda()\n",
        "\n",
        "        preds, aux = model(imgs)\n",
        "\n",
        "        loss = yolo_loss(preds, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | loss {total_loss:.4f}\")\n",
        "torch.save(model.state_dict(), \"hybrid_two_way_best.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=48,\n",
        "    embed_dim=512,\n",
        "    vit_depth=8,\n",
        "    vit_heads=8,\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=True\n",
        ").cuda()\n",
        "\n",
        "model.load_state_dict(torch.load(\"hybrid_two_way_best.pt\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "xMGJ7hNzVp1k"
      },
      "id": "xMGJ7hNzVp1k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1DpYDnPRgrE"
      },
      "source": [
        "## 7. Quick Sanity Check\n",
        "Colab에서 바로 실행해 모델 입출력 형태를 확인할 수 있습니다."
      ],
      "id": "U1DpYDnPRgrE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvJstXD8RgrE"
      },
      "outputs": [],
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=48,\n",
        "    embed_dim=512,\n",
        "    vit_depth=8,\n",
        "    vit_heads=8,\n",
        "    num_classes=1,\n",
        "    iters=1,\n",
        "    detach_feedback=True,\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640)\n",
        "preds, aux = model(x)\n",
        "for i, (c, o, b) in enumerate(preds, start=3):\n",
        "    print(f\"[TwoWay] P{i} cls:{list(c.shape)} obj:{list(o.shape)} box:{list(b.shape)}\")\n"
      ],
      "id": "rvJstXD8RgrE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}