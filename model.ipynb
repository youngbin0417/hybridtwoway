{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-EfYzNbTM1_"
      },
      "source": [
        "# HybridTwoWay Model (Colab Ready)\n"
      ],
      "id": "6-EfYzNbTM1_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMEtQnY6TM2A"
      },
      "source": [
        "## Imports\n",
        "ÌïÑÏöîÌïú PyTorch Î™®ÎìàÍ≥º ÌÉÄÏûÖ ÌûåÌä∏Î•º Î∂àÎü¨ÏòµÎãàÎã§."
      ],
      "id": "rMEtQnY6TM2A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bef1b69",
      "metadata": {
        "id": "8bef1b69"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib albumentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "77da3f4e",
      "metadata": {
        "id": "77da3f4e",
        "outputId": "33d9acff-7352-4896-e100-707e0580daa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in detection-base-6 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111006/111006 [00:02<00:00, 49870.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to detection-base-6 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3306/3306 [00:00<00:00, 5865.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roboflow dataset downloaded to: /content/detection-base-6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Cell 1: ÏÑ§Ïπò Î∞è Í∏∞Î≥∏ Import, Roboflow Îã§Ïö¥Î°úÎìú\n",
        "# ============================================\n",
        "\n",
        "import math\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from roboflow import Roboflow\n",
        "from tqdm import tqdm\n",
        "\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h63B30-bTM2B"
      },
      "source": [
        "## 0. Utility Functions\n",
        "## 1. Anomaly-Aware CNN Stem"
      ],
      "id": "h63B30-bTM2B"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z3ic6SvYTM2B"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2: Í∏∞Î≥∏ Conv Î∏îÎ°ù + Stem\n",
        "# ============================================\n",
        "\n",
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n",
        "\n",
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'),\n",
        "                        self.weight, groups=self.groups)\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "        self.base_ch = base_ch # Ï†ÄÏû•\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * self.base_ch\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ],
      "id": "Z3ic6SvYTM2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hro-tc1TM2B"
      },
      "source": [
        "## 2. Vision Transformer Encoder"
      ],
      "id": "1hro-tc1TM2B"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sCAhxcS_TM2B"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 3: ViT Encoder + Feedback Adapter\n",
        "# ============================================\n",
        "\n",
        "class PatchEmbed1x1(nn.Module):\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop_p = attn_drop\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (self.qkv(x)\n",
        "               .reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "               .permute(2, 0, 3, 1, 4))\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Flash Attention\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            dropout_p=self.attn_drop_p if self.training else 0.0,\n",
        "            scale=self.scale\n",
        "        )\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n",
        "\n",
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens, Ht, Wt, f_stem):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ],
      "id": "sCAhxcS_TM2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2rzunITM2B"
      },
      "source": [
        "## 4. PAN-Lite Neck"
      ],
      "id": "Vm2rzunITM2B"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3sVE1D7KTM2B"
      },
      "outputs": [],
      "source": [
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.down5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.up4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.up3 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse5 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        p3 = self.lateral(p3)\n",
        "        p4 = self.down4(p3)\n",
        "        p5 = self.down5(p4)\n",
        "        p4u = F.interpolate(p5, size=p4.shape[-2:], mode='nearest')\n",
        "        p4 = self.up4(torch.cat([p4, p4u], dim=1))\n",
        "        p3u = F.interpolate(p4, size=p3.shape[-2:], mode='nearest')\n",
        "        p3 = self.up3(torch.cat([p3, p3u], dim=1))\n",
        "        p4b = self.down_f4(p3)\n",
        "        p4 = self.fuse4(torch.cat([p4, p4b], dim=1))\n",
        "        p5b = self.down_f5(p4)\n",
        "        p5 = self.fuse5(torch.cat([p5, p5b], dim=1))\n",
        "        return p3, p4, p5"
      ],
      "id": "3sVE1D7KTM2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgqe2GZPTM2B"
      },
      "source": [
        "## 5. YOLO-style Detection Head"
      ],
      "id": "bgqe2GZPTM2B"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0ac62674",
      "metadata": {
        "id": "0ac62674"
      },
      "outputs": [],
      "source": [
        "class YOLOHeadLite(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "        self.stem3 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem5 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.cls3 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj3 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box3 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj3.bias, -4.59)\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj4.bias, -4.59)\n",
        "        self.cls5 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj5 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box5 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj5.bias, -4.59)\n",
        "\n",
        "    def forward_single(self, x, stem, cls, obj, box):\n",
        "        f = stem(x)\n",
        "        return cls(f), obj(f), box(f)\n",
        "\n",
        "    def forward(self, p3, p4, p5):\n",
        "        c3, o3, b3 = self.forward_single(p3, self.stem3, self.cls3, self.obj3, self.box3)\n",
        "        c4, o4, b4 = self.forward_single(p4, self.stem4, self.cls4, self.obj4, self.box4)\n",
        "        c5, o5, b5 = self.forward_single(p5, self.stem5, self.cls5, self.obj5, self.box5)\n",
        "        return [(c3, o3, b3), (c4, o4, b4), (c5, o5, b5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHQ-k5K2TM2C"
      },
      "source": [
        "## 6. HybridTwoWay Model"
      ],
      "id": "NHQ-k5K2TM2C"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X0YjWJOGTM2C"
      },
      "outputs": [],
      "source": [
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(self, in_ch=3, stem_base=32, embed_dim=256, vit_depth=4, vit_heads=4, num_classes=3, iters=1, detach_feedback=True, img_size=640):\n",
        "        super().__init__()\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4\n",
        "        self.patch = PatchEmbed1x1(c_stem, embed_dim)\n",
        "        self.num_patches = (img_size // 8) ** 2\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        self.vit = ViTEncoder(embed_dim=embed_dim, depth=vit_depth, num_heads=vit_heads)\n",
        "        self.feedback = FeedbackAdapter(d_token=embed_dim, c_stem=c_stem, use_bn=True)\n",
        "        self.neck = PANLite(in_ch=embed_dim, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_stem, vis = self.stem(x)\n",
        "        p0 = self.patch(f_stem)\n",
        "        Ht, Wt = p0.shape[-2:]\n",
        "        tokens = p0.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Positional Embedding (Dynamic Resizing)\n",
        "        if tokens.shape[1] != self.pos_embed.shape[1]:\n",
        "            pos_embed = F.interpolate(\n",
        "                self.pos_embed.reshape(1, int(self.num_patches**0.5), int(self.num_patches**0.5), -1).permute(0, 3, 1, 2),\n",
        "                size=(Ht, Wt), mode='bicubic', align_corners=False\n",
        "            ).flatten(2).transpose(1, 2)\n",
        "            tokens = tokens + pos_embed\n",
        "        else:\n",
        "            tokens = tokens + self.pos_embed\n",
        "\n",
        "        f_fb = f_stem\n",
        "        preds, aux = None, None\n",
        "\n",
        "        for i in range(self.iters):\n",
        "            tokens = self.vit(tokens)\n",
        "            toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "            f_fb = self.feedback(toks_for_fb, Ht, Wt, f_fb)\n",
        "            p3_in = self.patch(f_fb)\n",
        "            p3, p4, p5 = self.neck(p3_in)\n",
        "            preds = self.head(p3, p4, p5)\n",
        "            aux = {\"P3\": p3, \"P4\": p4, \"P5\": p5, \"V\": vis}\n",
        "            if i != self.iters - 1:\n",
        "                tokens = p3_in.flatten(2).transpose(1, 2)\n",
        "                # Re-add Pos Embed for next iter\n",
        "                tokens = tokens + (pos_embed if 'pos_embed' in locals() else self.pos_embed)\n",
        "        return preds, aux"
      ],
      "id": "X0YjWJOGTM2C"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b16ccb66",
      "metadata": {
        "id": "b16ccb66"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 6: Dataset / Dataloader\n",
        "# ============================================\n",
        "\n",
        "IMG_SIZE = 640\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # Ïù¥ÎØ∏ 640x640Ïù¥ÎØÄÎ°ú resizeÎäî Í∞ÄÎ≥çÍ≤å ÌÜµÍ≥º\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = torch.tensor(img).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, x, y, w, h = map(float, line.split())\n",
        "                    boxes.append([cls, x, y, w, h])\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        return img, boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "98c51e46",
      "metadata": {
        "id": "98c51e46"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 7: YOLO-style Loss (Focal + GIoU)\n",
        "# ============================================\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        prob = torch.sigmoid(logits)\n",
        "        ce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "        p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "        focal_term = (1 - p_t) ** self.gamma\n",
        "        loss = ce * focal_term\n",
        "        if self.alpha >= 0:\n",
        "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "            loss = alpha_t * loss\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        elif self.reduction == \"sum\": return loss.sum()\n",
        "        else: return loss\n",
        "\n",
        "def xywh_to_xyxy(box_xywh):\n",
        "    x_c, y_c, w, h = box_xywh.unbind(-1)\n",
        "    x1, y1 = x_c - w / 2, y_c - h / 2\n",
        "    x2, y2 = x_c + w / 2, y_c + h / 2\n",
        "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
        "\n",
        "\n",
        "def giou_loss(pred_box_xyxy, tgt_box_xyxy):\n",
        "    x1 = torch.max(pred_box_xyxy[:, 0], tgt_box_xyxy[:, 0])\n",
        "    y1 = torch.max(pred_box_xyxy[:, 1], tgt_box_xyxy[:, 1])\n",
        "    x2 = torch.min(pred_box_xyxy[:, 2], tgt_box_xyxy[:, 2])\n",
        "    y2 = torch.min(pred_box_xyxy[:, 3], tgt_box_xyxy[:, 3])\n",
        "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
        "    area_p = (pred_box_xyxy[:, 2]-pred_box_xyxy[:, 0]).clamp(min=0)*(pred_box_xyxy[:, 3]-pred_box_xyxy[:, 1]).clamp(min=0)\n",
        "    area_t = (tgt_box_xyxy[:, 2]-tgt_box_xyxy[:, 0]).clamp(min=0)*(tgt_box_xyxy[:, 3]-tgt_box_xyxy[:, 1]).clamp(min=0)\n",
        "    union = area_p + area_t - inter + 1e-6\n",
        "    iou = inter / union\n",
        "    c_x1 = torch.min(pred_box_xyxy[:, 0], tgt_box_xyxy[:, 0])\n",
        "    c_y1 = torch.min(pred_box_xyxy[:, 1], tgt_box_xyxy[:, 1])\n",
        "    c_x2 = torch.max(pred_box_xyxy[:, 2], tgt_box_xyxy[:, 2])\n",
        "    c_y2 = torch.max(pred_box_xyxy[:, 3], tgt_box_xyxy[:, 3])\n",
        "    c_area = (c_x2 - c_x1).clamp(min=0) * (c_y2 - c_y1).clamp(min=0) + 1e-6\n",
        "    giou = iou - (c_area - union) / c_area\n",
        "    return (1.0 - giou).mean()\n",
        "\n",
        "# focal loss Ïù∏Ïä§ÌÑ¥Ïä§ (object + class Îëò Îã§ ÏÇ¨Ïö©)\n",
        "_focal_loss = FocalLoss(alpha=0.25, gamma=2.0, reduction=\"mean\")\n",
        "\n",
        "def yolo_loss(preds, targets, img_size=512, lambda_obj=1.0, lambda_cls=1.0, lambda_box=5.0):\n",
        "    total_obj_loss, total_cls_loss, total_box_loss = 0.0, 0.0, 0.0\n",
        "\n",
        "    # Batch Size ÌôïÏù∏\n",
        "    B_size = preds[0][0].shape[0]\n",
        "\n",
        "    for scale_id, (cls_pred, obj_pred, box_pred) in enumerate(preds):\n",
        "        B, C, H, W = cls_pred.shape\n",
        "        device = cls_pred.device\n",
        "        cls_p = cls_pred.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
        "        obj_p = obj_pred.permute(0, 2, 3, 1).reshape(B, H * W, 1)\n",
        "        box_p = box_pred.permute(0, 2, 3, 1).reshape(B, H * W, 4)\n",
        "        stride = img_size // H\n",
        "\n",
        "        for b in range(B):\n",
        "            gt = targets[b]\n",
        "            if gt.numel() == 0:\n",
        "                obj_tgt = torch.zeros((H * W, 1), device=device)\n",
        "                total_obj_loss += _focal_loss(obj_p[b], obj_tgt)\n",
        "                continue\n",
        "\n",
        "            gcls = gt[:, 0].long()\n",
        "            gxy_norm = gt[:, 1:3]\n",
        "            gwh_norm = gt[:, 3:5]\n",
        "            gxy_pix = gxy_norm * img_size\n",
        "            gx = (gxy_pix[:, 0] / stride).long().clamp(0, W - 1)\n",
        "            gy = (gxy_pix[:, 1] / stride).long().clamp(0, H - 1)\n",
        "            gi = gy * W + gx\n",
        "\n",
        "            obj_tgt = torch.zeros((H * W, 1), device=device)\n",
        "            obj_tgt[gi] = 1.0\n",
        "            total_obj_loss += _focal_loss(obj_p[b], obj_tgt)\n",
        "\n",
        "            cls_tgt = torch.zeros((H * W, C), device=device)\n",
        "            cls_tgt[gi, gcls] = 1.0\n",
        "            total_cls_loss += _focal_loss(cls_p[b], cls_tgt)\n",
        "\n",
        "            pred_raw = box_p[b][gi]\n",
        "            pred_box_norm_xywh = pred_raw.sigmoid()\n",
        "            tgt_box_norm_xywh = torch.cat([gxy_norm, gwh_norm], dim=1)\n",
        "            pred_xyxy = xywh_to_xyxy(pred_box_norm_xywh)\n",
        "            tgt_xyxy = xywh_to_xyxy(tgt_box_norm_xywh)\n",
        "            total_box_loss += giou_loss(pred_xyxy, tgt_xyxy)\n",
        "\n",
        "    # Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶àÎ°ú ÎÇòÎàÑÏñ¥ Ï†ïÍ∑úÌôî (ÌïôÏäµ ÏïàÏ†ïÏÑ± ÌôïÎ≥¥)\n",
        "    total = (lambda_obj * total_obj_loss + lambda_cls * total_cls_loss + lambda_box * total_box_loss) / B_size\n",
        "    return total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "276bf933",
      "metadata": {
        "id": "276bf933"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 8: Decode Predictions + mAP Evaluation\n",
        "# ============================================\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    N = box1.size(0)\n",
        "    M = box2.size(0)\n",
        "    if N == 0 or M == 0: return torch.zeros(N, M)\n",
        "    tl = torch.max(box1[:, None, :2], box2[:, :2])\n",
        "    br = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
        "    wh = (br - tl).clamp(min=0)\n",
        "    inter = wh[..., 0] * wh[..., 1]\n",
        "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])\n",
        "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])\n",
        "    return inter / (area1[:, None] + area2 - inter + 1e-6)\n",
        "\n",
        "def nms(boxes, scores, iou_thres=0.5):\n",
        "    if boxes.numel() == 0: return torch.zeros(0, dtype=torch.long, device=boxes.device)\n",
        "    idxs = scores.argsort(descending=True)\n",
        "    keep = []\n",
        "    while idxs.numel() > 0:\n",
        "        i = idxs[0]\n",
        "        keep.append(i.item())\n",
        "        if idxs.numel() == 1: break\n",
        "        ious = box_iou(boxes[i].unsqueeze(0), boxes[idxs[1:]]).squeeze(0)\n",
        "        idxs = idxs[1:][ious < iou_thres]\n",
        "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "def decode_predictions(preds, img_size=512, conf_thres=0.25, nms_iou_thres=0.5):\n",
        "    all_outputs = []\n",
        "    B = preds[0][0].shape[0]\n",
        "    for b in range(B):\n",
        "        dets_all = []\n",
        "        for (cls_pred, obj_pred, box_pred) in preds:\n",
        "            B_s, C, H, W = cls_pred.shape\n",
        "            cls_logits = cls_pred[b].permute(1,2,0).reshape(H*W, C)\n",
        "            obj_logits = obj_pred[b].permute(1,2,0).reshape(H*W, 1)\n",
        "            box_logits = box_pred[b].permute(1,2,0).reshape(H*W, 4)\n",
        "            obj_scores = obj_logits.sigmoid().squeeze(-1)\n",
        "            cls_scores = cls_logits.sigmoid()\n",
        "            box_norm = box_logits.sigmoid()\n",
        "            cls_max_scores, cls_ids = cls_scores.max(dim=-1)\n",
        "            scores = obj_scores * cls_max_scores\n",
        "            mask = scores > conf_thres\n",
        "            if mask.sum() == 0: continue\n",
        "            scores_ = scores[mask]\n",
        "            cls_ids_ = cls_ids[mask]\n",
        "            boxes = box_norm[mask]\n",
        "            x_c, y_c, w, h = boxes[:, 0]*img_size, boxes[:, 1]*img_size, boxes[:, 2]*img_size, boxes[:, 3]*img_size\n",
        "            x1, y1 = (x_c - w/2).clamp(0, img_size), (y_c - h/2).clamp(0, img_size)\n",
        "            x2, y2 = (x_c + w/2).clamp(0, img_size), (y_c + h/2).clamp(0, img_size)\n",
        "            boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "            keep = nms(boxes_xyxy, scores_, iou_thres=nms_iou_thres)\n",
        "            if keep.numel() == 0: continue\n",
        "            dets = torch.cat([boxes_xyxy[keep], scores_[keep].unsqueeze(1), cls_ids_[keep].float().unsqueeze(1)], dim=1)\n",
        "            dets_all.append(dets)\n",
        "        all_outputs.append(torch.cat(dets_all, dim=0) if len(dets_all) > 0 else [])\n",
        "    return all_outputs\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    mrec = torch.cat([torch.tensor([0.0]), recall, torch.tensor([1.0])])\n",
        "    mpre = torch.cat([torch.tensor([0.0]), precision, torch.tensor([0.0])])\n",
        "    for i in range(mpre.size(0)-1, 0, -1):\n",
        "        mpre[i-1] = torch.max(mpre[i-1], mpre[i])\n",
        "    idx = (mrec[1:] != mrec[:-1]).nonzero().squeeze()\n",
        "    return ((mrec[idx+1] - mrec[idx]) * mpre[idx+1]).sum().item()\n",
        "\n",
        "def evaluate_map(model, dataloader, num_classes=3, img_size=512, iou_thr=0.5, conf_thres=0.25):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_dets = {c: [] for c in range(num_classes)}\n",
        "    all_gts  = {c: [] for c in range(num_classes)}\n",
        "    global_img_id = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            targets = [t.to(device) for t in targets]\n",
        "            preds, _ = model(imgs)\n",
        "            dets_list = decode_predictions(preds, img_size=img_size, conf_thres=conf_thres)\n",
        "\n",
        "            for b in range(len(imgs)):\n",
        "                dets = dets_list[b]\n",
        "                gt = targets[b]\n",
        "                current_img_id = global_img_id\n",
        "                global_img_id += 1\n",
        "\n",
        "                if len(gt) > 0:\n",
        "                    gcls = gt[:, 0].long()\n",
        "                    gxy, gwh = gt[:, 1:3] * img_size, gt[:, 3:5] * img_size\n",
        "                    gx1, gy1 = gxy[:, 0] - gwh[:, 0]/2, gxy[:, 1] - gwh[:, 1]/2\n",
        "                    gx2, gy2 = gxy[:, 0] + gwh[:, 0]/2, gxy[:, 1] + gwh[:, 1]/2\n",
        "                    gboxes = torch.stack([gx1, gy1, gx2, gy2], dim=1)\n",
        "                    for c in range(num_classes):\n",
        "                        mask = (gcls == c)\n",
        "                        if mask.sum() > 0: all_gts[c].append((current_img_id, gboxes[mask].cpu()))\n",
        "\n",
        "                if dets is not None and len(dets) > 0:\n",
        "                    boxes, scores, cls_ids = dets[:, :4], dets[:, 4], dets[:, 5].long()\n",
        "                    for c in range(num_classes):\n",
        "                        mask = (cls_ids == c)\n",
        "                        if mask.sum() > 0: all_dets[c].append((current_img_id, scores[mask].cpu(), boxes[mask].cpu()))\n",
        "\n",
        "    aps = []\n",
        "    for c in range(num_classes):\n",
        "        gts_c = all_gts[c]\n",
        "        if len(gts_c) == 0: continue\n",
        "        n_gt = sum(boxes.size(0) for _, boxes in gts_c)\n",
        "        gt_dict = {}\n",
        "        for img_id, boxes in gts_c:\n",
        "            gt_dict.setdefault(img_id, [])\n",
        "            gt_dict[img_id].append({\"boxes\": boxes, \"matched\": torch.zeros(boxes.size(0), dtype=torch.bool)})\n",
        "\n",
        "        dets_c = all_dets[c]\n",
        "        if len(dets_c) == 0:\n",
        "            aps.append(0.0)\n",
        "            continue\n",
        "\n",
        "        scores_all, boxes_all, img_ids_all = [], [], []\n",
        "        for img_id, scores, boxes in dets_c:\n",
        "            for i in range(boxes.size(0)):\n",
        "                scores_all.append(scores[i].item())\n",
        "                boxes_all.append(boxes[i])\n",
        "                img_ids_all.append(img_id)\n",
        "\n",
        "        scores_all = torch.tensor(scores_all)\n",
        "        boxes_all = torch.stack(boxes_all, dim=0)\n",
        "        order = scores_all.argsort(descending=True)\n",
        "        scores_all, boxes_all = scores_all[order], boxes_all[order]\n",
        "        img_ids_all = [img_ids_all[i] for i in order]\n",
        "\n",
        "        tps, fps = torch.zeros(len(scores_all)), torch.zeros(len(scores_all))\n",
        "        for i in range(len(scores_all)):\n",
        "            img_id = img_ids_all[i]\n",
        "            pred_box = boxes_all[i].unsqueeze(0)\n",
        "            if img_id not in gt_dict:\n",
        "                fps[i] = 1; continue\n",
        "            gt_entry = gt_dict[img_id][0]\n",
        "            ious = box_iou(pred_box, gt_entry[\"boxes\"]).squeeze(0)\n",
        "            if ious.numel() == 0: fps[i] = 1; continue\n",
        "            max_iou, max_idx = ious.max(0)\n",
        "            if max_iou >= iou_thr and not gt_entry[\"matched\"][max_idx]:\n",
        "                tps[i] = 1; gt_entry[\"matched\"][max_idx] = True\n",
        "            else: fps[i] = 1\n",
        "\n",
        "        tp_cum, fp_cum = torch.cumsum(tps, dim=0), torch.cumsum(fps, dim=0)\n",
        "        recall = tp_cum / (n_gt + 1e-6)\n",
        "        precision = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
        "        aps.append(compute_ap(recall, precision))\n",
        "\n",
        "    mAP = sum(aps) / len(aps) if len(aps) > 0 else 0.0\n",
        "    return mAP, aps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "417b2f55",
      "metadata": {
        "id": "417b2f55",
        "outputId": "142ad0ce-764d-403c-ee07-1ae91ebbcbb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = dataset.location\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"))\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"))\n",
        "test_dataset  = YoloDataset(os.path.join(DATA_PATH, \"test\"))\n",
        "# [ÏµúÏ†ÅÌôî] num_workers=2, pin_memory=True Ï∂îÍ∞Ä\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9a2f274e",
      "metadata": {
        "id": "9a2f274e",
        "outputId": "08533ea4-5fdb-4c15-a345-51d2457f6f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model compiled with torch.compile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# ÌïôÏäµ Ï§ÄÎπÑ\n",
        "# ============================================\n",
        "cfg = dict(\n",
        "    in_ch=3,\n",
        "    stem_base=32,\n",
        "    embed_dim=256,\n",
        "    vit_depth=4,\n",
        "    vit_heads=4,\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=True,\n",
        "    img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "model = HybridTwoWay(**cfg).to(device)\n",
        "\n",
        "# [ÏÑ†ÌÉù] torch.compile (PyTorch 2.0+, Colab T4/L4)\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print(\"‚úÖ Model compiled with torch.compile\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è torch.compile failed/skipped\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "EPOCHS = 10\n",
        "\n",
        "best_map = -1.0\n",
        "best_epoch = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3b18a0c7",
      "metadata": {
        "id": "3b18a0c7",
        "outputId": "c457acec-b5c8-4735-fc19-cde9daa80daa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max allocated: 1.306610107421875 GB\n",
            "Max reserved : 2.177734375 GB\n"
          ]
        }
      ],
      "source": [
        "# VRAM ÏÇ¨Ïö©Îüâ\n",
        "imgs, targets = next(iter(train_loader))\n",
        "imgs = imgs.to(device)\n",
        "targets = [t.to(device) for t in targets]\n",
        "\n",
        "optimizer.zero_grad()\n",
        "with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "    preds, aux = model(imgs)\n",
        "    loss = yolo_loss(preds, targets, img_size=IMG_SIZE)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
        "print(\"Max reserved :\", torch.cuda.max_memory_reserved() / 1024**3, \"GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "63ce3543",
      "metadata": {
        "id": "63ce3543",
        "outputId": "80fbc59f-e1ab-4117-a792-af57e3bfe951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Start Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:04<00:00,  2.67it/s, loss=11.1724]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Average Loss: 12.6366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val mAP@0.5: 0.0147\n",
            "‚úÖ Best model saved! (Val mAP: 0.0147)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:04<00:00,  2.69it/s, loss=8.9998]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Average Loss: 10.4648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val mAP@0.5: 0.0267\n",
            "‚úÖ Best model saved! (Val mAP: 0.0267)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.72it/s, loss=10.2649]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Average Loss: 9.9766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val mAP@0.5: 0.0240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.71it/s, loss=11.3181]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Average Loss: 9.6514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val mAP@0.5: 0.0241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:04<00:00,  2.70it/s, loss=10.1903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Average Loss: 9.3809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val mAP@0.5: 0.0216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.72it/s, loss=10.5236]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Average Loss: 9.1112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val mAP@0.5: 0.0277\n",
            "‚úÖ Best model saved! (Val mAP: 0.0277)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.71it/s, loss=8.3596]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Average Loss: 8.8977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val mAP@0.5: 0.0268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.73it/s, loss=6.0074]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Average Loss: 8.7006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val mAP@0.5: 0.0289\n",
            "‚úÖ Best model saved! (Val mAP: 0.0289)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.72it/s, loss=9.1786]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Train Average Loss: 8.5025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Val mAP@0.5: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:03<00:00,  2.72it/s, loss=8.0174]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Average Loss: 8.2511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Val mAP@0.5: 0.0219\n",
            "üèÅ Training Finished!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Cell 9: ÌïôÏäµ Î£®ÌîÑ (AMP + Flash Attention + PosEmbed)\n",
        "# ============================================\n",
        "\n",
        "print(\"üöÄ Start Training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for imgs, targets in loop:\n",
        "        imgs = imgs.to(device)\n",
        "        targets = [t.to(device) for t in targets]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            preds, aux = model(imgs)\n",
        "            loss = yolo_loss(preds, targets, img_size=IMG_SIZE)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    val_map, val_aps = evaluate_map(model, val_loader, num_classes=cfg[\"num_classes\"], img_size=IMG_SIZE, conf_thres=0.01)\n",
        "    print(f\"Epoch {epoch+1} | Val mAP@0.5: {val_map:.4f}\")\n",
        "\n",
        "    if val_map > best_map:\n",
        "        best_map = val_map\n",
        "        best_epoch = epoch + 1\n",
        "        # compile ÏÇ¨Ïö© Ïãú ÏõêÎ≥∏ state_dict Ï†ÄÏû•ÏùÑ ÏúÑÌï¥ _orig_mod ÌôïÏù∏\n",
        "        state_dict = model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict()\n",
        "        ckpt = {\"state_dict\": state_dict, \"cfg\": cfg, \"epoch\": best_epoch, \"val_map\": best_map}\n",
        "        torch.save(ckpt, \"hybrid_two_way_best.pt\")\n",
        "        print(f\"‚úÖ Best model saved! (Val mAP: {best_map:.4f})\")\n",
        "\n",
        "print(\"üèÅ Training Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "dded0d33",
      "metadata": {
        "id": "dded0d33",
        "outputId": "f5ee2570-5124-4036-c641-e820ada7001a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ Final Test mAP@0.5: 0.0402\n",
            "   Class 0 AP@0.5: 0.0508\n",
            "   Class 1 AP@0.5: 0.0693\n",
            "   Class 2 AP@0.5: 0.0004\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Ï†ÄÏû•Îêú Best Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ Î∞è ÌÖåÏä§Ìä∏ ÌèâÍ∞Ä\n",
        "# ============================================\n",
        "\n",
        "checkpoint = torch.load(\"hybrid_two_way_best.pt\", map_location=device)\n",
        "loaded_cfg = checkpoint[\"cfg\"]\n",
        "model = HybridTwoWay(**loaded_cfg).to(device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "test_map, class_aps = evaluate_map(model, test_loader, num_classes=loaded_cfg[\"num_classes\"], img_size=IMG_SIZE, conf_thres=0.01)\n",
        "print(f\"\\nüèÜ Final Test mAP@0.5: {test_map:.4f}\")\n",
        "for i, ap in enumerate(class_aps):\n",
        "    print(f\"   Class {i} AP@0.5: {ap:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YSWsLFYFTM2D",
        "outputId": "1f1f7658-cc52-4226-ee81-db344528f6e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[P3] cls: [2, 3, 80, 80], obj: [2, 1, 80, 80], box: [2, 4, 80, 80]\n",
            "[P4] cls: [2, 3, 40, 40], obj: [2, 1, 40, 40], box: [2, 4, 40, 40]\n",
            "[P5] cls: [2, 3, 20, 20], obj: [2, 1, 20, 20], box: [2, 4, 20, 20]\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Cell 10: Quick Sanity Check (ÏûÖÏ∂úÎ†• shape ÌôïÏù∏)\n",
        "# ============================================\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640).to(device)\n",
        "preds, aux = model(x)\n",
        "\n",
        "for level, (c, o, b) in zip([\"P3\",\"P4\",\"P5\"], preds):\n",
        "    print(f\"[{level}] cls: {list(c.shape)}, obj: {list(o.shape)}, box: {list(b.shape)}\")\n"
      ],
      "id": "YSWsLFYFTM2D"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}